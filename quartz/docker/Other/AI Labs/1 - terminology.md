
Pre-trained model - model that has been trained on a large dataset for a specific task before being made available for use. 

Inference - inference is the process of using a trained model predictions or draw conclusions about new unseen data based on the learned patterns from the training data.

Transformers - models that handle text based tasks (special library that relies on attention mechanisms)

Tokenizer - process that breaks down text into smaller units called tokens 




